{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdfkajAsv7ib",
        "outputId": "22708a8d-8ed0-41b1-8526-91e4dadfeee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SparkSession:\n",
        "SparkSession is a newer, unified entry point introduced in Spark 2.x that combines both SparkContext and SQLContext (which were previously used separately).\n",
        "It provides a higher-level API for working with Spark, particularly for handling DataFrame and SQL operations.\n",
        "SparkSession is designed to work with structured data (DataFrames and Datasets), which is a more abstract and optimized API compared to RDDs.\n",
        "It internally manages a SparkContext instance, so you can access it via spark.sparkContext."
      ],
      "metadata": {
        "id": "qqLgpyro61Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Learning PySpark\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "P6mG0CyPwMa2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(\"Alice\",1),(\"bob\",2),(\"charlie\",3)]\n",
        "columns= [\"Name\",\"Value\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7w6NYU6wW1G",
        "outputId": "a066fee5-4bc3-4dc6-f3b5-9ec8a69a0d90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|  Alice|    1|\n",
            "|    bob|    2|\n",
            "|charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"value\"]>1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6zvSrtmzRwT",
        "outputId": "cbee65c3-510d-4c39-ad51-e30a18d54fa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|    bob|    2|\n",
            "|charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"value\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E7F7PbNzrBC",
        "outputId": "c10bc8a3-54b6-4be0-9dbb-87714e0e1f11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|value|count|\n",
            "+-----+-----+\n",
            "|    1|    1|\n",
            "|    3|    1|\n",
            "|    2|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"Double\", df[\"value\"] * 2)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PMpZNdX1zEG",
        "outputId": "6f3300c5-c0e5-4b23-9626-95a1ceaa17c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------+\n",
            "|   Name|Value|Double|\n",
            "+-------+-----+------+\n",
            "|  Alice|    1|     2|\n",
            "|    bob|    2|     4|\n",
            "|charlie|    3|     6|\n",
            "+-------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can join two DataFrames based on common columns"
      ],
      "metadata": {
        "id": "HuvCMdih2WEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame\n",
        "data2 = [(\"Alice\", \"NY\"), (\"Bob\", \"LA\")]\n",
        "columns2 = [\"Name\", \"City\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "joined_df =df.join(df2,\"Name\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnwX7j_o2zMz",
        "outputId": "130c72fd-bb70-4a19-c4d7-dfe3cf249603"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------+----+\n",
            "| Name|Value|Double|City|\n",
            "+-----+-----+------+----+\n",
            "|Alice|    1|     2|  NY|\n",
            "+-----+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Parquet files include schema information (metadata) along with the data, which defines the structure of the data (e.g., field names, data types). This makes it self-descriptive, allowing for easier integration and querying across different tools.\n",
        "2.   Parquet stores data in a columnar format, which means that data is stored by column rather than by row. This allows for better compression and faster read performance, especially when you only need to access a subset of the columns in a large dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "6aBHszKW4dR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame as Parquet\n",
        "df.write.parquet(\"output.parquet\")"
      ],
      "metadata": {
        "id": "d-gtU4Z14cXe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic SQL Operations.."
      ],
      "metadata": {
        "id": "YV8tZmCY5x4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"people\")\n"
      ],
      "metadata": {
        "id": "O0ThD1S950iR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute SQL query\n",
        "result = spark.sql(\"SELECT Name, Value FROM global_temp.people WHERE Value > 1\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYRCQ2SX6L_v",
        "outputId": "87d577da-a1d7-4b07-b0f7-48db1d4cc544"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|    bob|    2|\n",
            "|charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD is a fundamental data structure in Spark and is the backbone of Spark's distributed computing capabilities. RDDs allow you to perform distributed data processing and are fault-tolerant, meaning that if any partition of the RDD is lost due to a node failure, it can be recomputed."
      ],
      "metadata": {
        "id": "NJk2jMDx7REm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"RDD Basics\").getOrCreate()\n",
        "\n",
        "# Access the SparkContext from the SparkSession\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "DzohWmJh6yxH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The sc.parallelize function in Apache Spark is used to distribute a local collection (e.g., a list or an array) across multiple nodes in a cluster, creating an RDD (Resilient Distributed Dataset). It enables you to perform parallel operations on data in a distributed manner."
      ],
      "metadata": {
        "id": "htyQn1tt7tug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [1,2,3,4,5]\n",
        "rdd = sc.parallelize(data)\n"
      ],
      "metadata": {
        "id": "NAzvfVrs7ixG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Collect the RDD to check its contents"
      ],
      "metadata": {
        "id": "dTLAVN7d9VEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p97NGV99WkV",
        "outputId": "a3a60ab2-f19d-4dbd-83c2-406428e38406"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_file = sc.textFile(\"output.parquet\")\n",
        "rdd_file.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0wiTl1V9cFA",
        "outputId": "08379ee8-1c16-4122-cd63-916a15bad696"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PAR1\\x15\\x00\\x15B\\x15F\\x15Ǹ��\\x02\\x1c\\x15\\x06\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00!�\\x02\\x00\\x00\\x00\\x03\\x07\\x05\\x00\\x00\\x00Alice\\x03\\x00\\x00\\x00bob\\x07\\x00\\x00\\x00charlie\\x15\\x00\\x15<\\x156\\x15����\\x05\\x1c\\x15\\x06\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x1e\\x1c\\x02\\x00\\x00\\x00\\x03\\x07\\x01\\x00\\t\\x01\\x01\\x0e,\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15<\\x15:\\x15���8\\x1c\\x15\\x06\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x1e\\x14\\x02\\x00\\x00\\x00\\x03\\x07\\x01\\x06\\x01\\x01<\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x05Alice\\x19\\x18\\x07charlie\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15t\\x16\\x00\\x00\\x00\\x19\\x1c\\x16|\\x15d\\x16\\x00\\x00\\x00\\x19\\x1c\\x16�\\x01\\x15f\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x0c%\\x02\\x18\\x04Name%\\x00L\\x1c\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x05Value\\x00\\x15\\x04%\\x02\\x18\\x06Double\\x00\\x16\\x06\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x0c\\x195\\x08\\x00\\x06\\x19\\x18\\x04Name\\x15\\x02\\x16\\x06\\x16p\\x16t&\\x08<6\\x00(\\x07charlie\\x18\\x05Alice\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16�\\x03\\x15\\x14\\x16�\\x02\\x156\\x00&|\\x1c\\x15\\x04\\x195\\x08\\x00\\x06\\x19\\x18\\x05Value\\x15\\x02\\x16\\x06\\x16j\\x16d&|<\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16�\\x04\\x15\\x14\\x16�\\x02\\x15>\\x00&�\\x01\\x1c\\x15\\x04\\x195\\x08\\x00\\x06\\x19\\x18\\x06Double\\x15\\x02\\x16\\x06\\x16h\\x16f&�\\x01<\\x18\\x08\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16�\\x04\\x15\\x16\\x16�\\x03\\x15>\\x00\\x16�\\x02\\x16\\x06&\\x08\\x16�\\x02\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.5.3\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18�\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"Name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Value\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Double\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00�\\x02\\x00\\x00PAR1']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD Transformations\n",
        "Transformations are operations that create a new RDD by applying a function to each element of the existing RDD. They are lazy, meaning the computation is not performed until an action is called."
      ],
      "metadata": {
        "id": "zicPpPx897EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_map = rdd.map(lambda x: x*2)\n",
        "rdd_map.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OuS4olo93F1",
        "outputId": "1b3d63de-2fd3-43e2-8d14-f0e7cc34cdc5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_filter=rdd.filter(lambda x: x%2==0)\n",
        "rdd_filter.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3x7ezor-Owi",
        "outputId": "46d73cf6-a6e6-4bee-8512-92ef028ae959"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flatMap() – Similar to map(), but the output can be a list of items, effectively flattening the result:\n"
      ],
      "metadata": {
        "id": "kr6H8lMl-jDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each element into multiple words\n",
        "rdd_flatmap = sc.parallelize([\"hello world\", \"apache spark\"]).flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "# Collect and show the result\n",
        "print(rdd_flatmap.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kuhnQfj-ijR",
        "outputId": "e6bf80a1-b72c-41f1-9267-8067fdfa2acf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'apache', 'spark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduce() – Reduces the RDD to a single value by applying a function to combine elements:"
      ],
      "metadata": {
        "id": "cr9cgEEC_CRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the elements of the RDD\n",
        "sum_result = rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "# Show the result\n",
        "print(sum_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leRIwOoj-a3N",
        "outputId": "1b208a90-f1fd-406f-e6f6-c3902bf54e8b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD Actions\n",
        "Actions are operations that trigger the execution of the transformations and return a value or export data."
      ],
      "metadata": {
        "id": "1xUFBrMw_N03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the RDD data\n",
        "collected_data = rdd.collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "T63NnJXQ_M6T"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the result\n",
        "print(collected_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0B1WVwG_a-U",
        "outputId": "5c9ce452-723c-411d-9578-650334f5946f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of elements in the RDD\n",
        "count_result = rdd.count()\n",
        "\n",
        "# Show the result\n",
        "print(count_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiRtrU3h_hly",
        "outputId": "63d03130-3485-448a-8760-2f0cce016613"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first element in the RDD\n",
        "first_element = rdd.first()\n",
        "\n",
        "# Show the result\n",
        "print(first_element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwKtItv__qxU",
        "outputId": "fa932bd9-bbc6-4305-d5fa-7b0865510aac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 3 elements in the RDD\n",
        "first_three = rdd.take(3)\n",
        "\n",
        "# Show the result\n",
        "print(first_three)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQlT7S7i_l4I",
        "outputId": "8dd5972c-fa1b-4b89-ab6e-cc79f3c608c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD Persistence\n",
        "You can cache or persist an RDD to store it in memory for efficient reuse. This is useful if you plan to perform multiple actions on the same RDD.\n",
        "\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "ITDYAafZ_ynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the RDD to memory\n",
        "rdd.cache()\n",
        "\n",
        "# Now, any subsequent action on rdd will be faster due to caching\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHVOP4Gp_qAs",
        "outputId": "7364d258-4789-420a-f71b-5fc5cdacda3d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[38] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "groupByKey() groups all values for each key.\n",
        "\n",
        "reduceByKey() performs a reduction operation on the values for each key (  **efficient**)."
      ],
      "metadata": {
        "id": "_f8oHo9CBYMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd= sc.parallelize([(\"a\",1),(\"b\",1),(\"a\",1),(\"b\",1)])\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "grouped_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahk0Y6NuBZC9",
        "outputId": "1b466212-5af0-44e8-c0ed-e420395e9e15"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', <pyspark.resultiterable.ResultIterable at 0x789f59242200>),\n",
              " ('b', <pyspark.resultiterable.ResultIterable at 0x789f59243a60>)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using reduceByKey: Aggregates the values for each key (more efficient)\n",
        "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
        "print(reduced_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ38exZrCNim",
        "outputId": "5750c717-9a80-4970-82cf-8ae638df2a91"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 2), ('b', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lN5btcE1OXnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "groupByKey() will return a list of values for each key, whereas\n",
        "\n",
        " reduceByKey() will combine the values into a single value per key, based on the function provided."
      ],
      "metadata": {
        "id": "69_cnM70CV5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Repartitioning and Coalescing RDDs\n",
        "repartition(): Increases the number of partitions.\n",
        "coalesce(): Reduces the number of partitions (more efficient when reducing)."
      ],
      "metadata": {
        "id": "BY2ainhOOZIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Partitioning Matters: When data is distributed across partitions, operations like map() and reduce() can be executed in parallel across the partitions, improving performance.\n",
        "\n",
        "\n",
        "However, too many partitions may result in excessive overhead, while too few may not take advantage of parallelism"
      ],
      "metadata": {
        "id": "eZexCUeLOh0P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2mzbHd2CUxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}